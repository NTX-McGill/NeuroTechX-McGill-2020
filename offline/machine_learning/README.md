# Machine Learning

## Dependencies
* [Python 3.7.6](https://www.python.org/downloads/) or later
* [Numpy 1.18.1](https://numpy.org/) or later
* [Matplotlib 3.1.3](https://matplotlib.org/) or later
* [Pandas 1.0.3](https://pandas.pydata.org/) or later
* [Scipy 1.4.1](https://www.scipy.org/) or later
* [Scikit-learn 0.22.1](https://scikit-learn.org/stable/) or later
* [Seaborn 0.10.0](https://seaborn.pydata.org/) or later

## Classification Problem
We defined our classification problem to be a 10-class problem, where there were 9 classes for finger movement (representing all fingers excluding 1 thumb) and 1 class for no movement.

## Window Sizes
We used different rolling window sizes of 200ms, 500ms, 1000ms, and noticed that increasing from 200ms to 500ms considerably improved accuracy, with the tradeoff of a noticeable response lag. Increasing from 500ms to 1000ms only gave a slight improvement to accuracy, likely because longer windows may accidentally capture several keypresses instead of one.

While we could have asked users to type slower, this would have made the product less useable, so we stuck to 500ms windows as the standard for typical trials and let the text prediction help supply additional information. Note that when we had good trials, we brought this back down to 200ms.

## A Comprehensive List of Features Used

### Time-Series Features
* IEMG : the sum of the absolute value of the signal.
* MAV : the average absolute value of the amplitude of the signal.
* MMAV : the weighted average absolute value of the amplitude of the signal.
* VAR : the variance of the amplitude of the signal.
* VAR_ABS : the variance of the absolute value of the amplitude of the signal.
* RMS : (Root Mean Square) the square root of the average of the square of the amplitude of the signal.
* RMS_3 : splits the window into 3 equal sub-windows and computes RMS for each part.
* WL : (Waveform Length) the cumulative length of the waveform over the window.
* ZC :  (Zero-crossings) the number of times the signal crosses the x-axis and the distance between the two points on either side of thte crossing is greater than some threshold.
* SSC : (Slope Sign Change) the number of times the sign of the slope changes.
* WAMP : (Willison Amplitude) the number of times the change in amplitude between two consecutive points exceeds some threshold.

### Frequency domain Features
* FREQ_FEATS : the mean Power Spectral Density (PSD) of the 5-50 Hz frequency band divided into sub-bands of length 5 Hz.
* FREQ_FEATS_RELATIVE : the mean Power Spectral Density (PSD) of the 5-50 Hz frequency band divided into sub-bands of length 5 Hz, normalized by the PSD of the window
* FREQ_FEATS_MIN_MAX : the minimum and maximum PSD in the 5-50 Hz frequency band divided into sub-bands of length 5 Hz.
* FREQ_VAR : the variance of the 5-50 Hz frequency band divided into sub-bands of length 5 Hz.
* FREQ_MISC : SSC, MAV, MMAV, and ZC applied to the Fourier transform of the signal.

## Models Tested and Results
We explored several models, including Random Forests (RF), SVM, KKN, and Logistic Regression (LR).
We found that RF always seemed to perform slightly worse than KNN and LR, while SVM was not robust, had a long training time, and was not well-suited for the data.

In the end, the best performing models were KNN and LR. 
Each of these models consistently performed to acheive 70%+ accuracy with minimal tuning on regular trials. 
KNN achieved peak accuracy of 93.0% on 200ms good trials; however, LR converged much quicker, making it our go-to for rapid prototyping and experimenting with different features.

![Table showing KNN, LR, and CART results based on trial lengths](https://github.com/NTX-McGill/NeuroTechX-McGill-2020/blob/main/offline/machine_learning/classification_results_table.png)

## Confusion Matrix
Our confusion matrix revealed that the largest sources of error were cross-finger confusion, namely, pinkie-index confusion. This is likely due to the placement of the electrode tracking the index finger, since it was on a muscle on the outer side of the lower forearm.

![Confusion Matrix Matched with Heatmap](https://github.com/NTX-McGill/NeuroTechX-McGill-2020/blob/main/offline/machine_learning/confusion_matrix_and_heatmap.png)

A: Confusion matrices of KNN models trained on 500 ms windows of data from all the trials (top) and 200 ms windows from only the “good” trials (bottom).
B: Comparison between probability heatmap produced by the KNN model trained on the 500 ms, all trials dataset (top) and actual finger classes (bottom).

---

# Prediction Heatmaps
Prediction heatmaps, generated by `simulate_prediction.py` empowered us with a tool for three important parts of our process:

### 1. Visual Verification for Expected Model Behaviour
We aligned heatmaps with the corresponding signal (power not normalized) to quickly validate if the predictions the model was doing were reasonable. This was done to ensure that the model was making predictions based on signal character and not based off overfitting or by coincidence.

![Sample output with 2 heatmaps and 2 line graphs for signals](https://github.com/NTX-McGill/NeuroTechX-McGill-2020/blob/main/offline/machine_learning/simulate_prediction_sample_output.jpg)

Note that the x-axis for the line plots are based on indices of the segment of the EMG time-series data from the trial, while the x-axis for the heatmaps are based off of indices of the sliding window for predictions. These numbers enable us to map back the segment to the data used for further analysis (i.e. aren't significant for graphical purposes).

### 2. Deepening Understanding of Mislabeled Points

#### 2a. Giving more insight into commonly confused fingers:
In our confusion matrix, we understood that some fingers were occasionally confused with each other. For example, the pinkie would sometimes be labelled as index, due to electrode placement. With the heatmap, we can dig up explicit examples of when this happened, and give it deeper consideration.

#### 2b. Learning about the limitations of our model:
Heatmaps helped us refine our window size needed for prediction, as well as upper bounds on typing speed. This greatly supplemented the information we were getting based on the accuracies of the models given different varients of the data.
For example, in an extreme case of rapid typing, we can observe heatmaps like the one below:

![Poor predictions observed with fast typing](https://github.com/NTX-McGill/NeuroTechX-McGill-2020/blob/main/offline/machine_learning/simulate_prediction_rapid.jpg)

### 3. Prototyping Live Simulated Predictions with Software
Creating `simulate_prediction.py` was a milestone in realizing that heatmaps could be an incredibly useful tool for live analysis. Heatmaps of a similar style were produced on the dashboard to be displayed when the EMG armbands were responding to muscle potentials in real time. This provided a valuable form of neurofeedback to users, who could learn techniques to control the device with greater success (e.g. learning to make larger motions when typing with certain fingers or other needs based on the individual's unique physiology).

